# -*- coding: utf-8 -*-
"""randomslopes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19QLasETn3FNbTl6TkwPUiDg4bkkXLMxi
"""
import torch
import numpy as np
import cvxpy as cp
from numpy.linalg import inv, norm
from scipy.linalg import sqrtm
import pandas as pd
from sklearn.linear_model import LinearRegression
from plotnine import *
import statsmodels.api as sm

linear_model = LinearRegression()

import numpy as np
import pandas as pd

def tmat(X, f, k=None):
    """
    X: nxp matrix, including intercept if appropriate
    f: a factor variable of length n with k classes, stored as an integer
    k: number of classes, optional, defaults to maximum value in f

    Returns a matrix where each row corresponds to a level of the factor f,
    and each column corresponds to the sum of X for that level of f.
    """
    # Reshape X to be a matrix if X is a vector

    # Convert to pandas DataFrame for easy manipulation
    data = pd.DataFrame(X)
    data['f'] = f

    # Calculate the sums of X for each level of f
    psum = np.array(data.groupby('f').sum())

    return psum

def div(u, v):
    return np.where(v == 0, 0, u / v)

X_a = X
X_b = X
fa = f1
fb = f2

def method_of_moments_diagonal(y, X, X_a, X_b, fa, fb):
    n = len(y)
    p_a = X_a.shape[1]
    p_b = X_b.shape[1]
    p = X.shape[1] - 1
    C = len(np.unique(fb))
    R = len(np.unique(fa))

    W_a = np.column_stack([X_a[:, i] * X_a[:, j] for i in range(p_a) for j in range(p_a)])
    Za = tmat(W_a, fa)

    W_b = np.column_stack([X_b[:, i] * X_b[:, j] for i in range(p_b) for j in range(p_b)])
    Zb = tmat(W_b, fb)


    t_a = np.arange(0, p_a**2, p_a + 1)
    t_b = np.arange(0, p_b**2, p_b + 1)

    sum_xa_2 = Za[:, t_a].sum(axis=0)
    sum_xb_2 = Zb[:, t_b].sum(axis=0)

    # print(sum_xa_2)
    left = np.zeros((p_a, p_a))
    for i in range(p_a):
        a = np.zeros(p_a)
        for i1 in range(p_a):
            a[i1] = sum_xa_2[i1] - np.sum(div(Za[:, (i*p_a + i1)]**2, Za[:, (i*p_a + i)]))
        left[i, :] = a

    right = np.zeros((p_a, p_b))
    for i in range(p_a):
        a = np.zeros(p_b)
        for i1 in range(p_b):
            a[i1] = sum_xb_2[i1] - np.sum(div(tmat((X_a[:, i] * X_b[:, i1])**2, fa).squeeze(), Za[:, (i*p_a + i)]))
        right[i, :] = a

    mat = np.hstack((left, right, (n - R)*(np.ones((p_a, 1)))))

    left = np.zeros((p_b, p_a))
    for i in range(p_b):
        a = np.zeros(p_a)
        for i1 in range(p_a):
            a[i1] = sum_xa_2[i1] - np.sum(div(tmat((X_b[:, i] * X_a[:, i1])**2, fb).squeeze(), Zb[:, (i*p_b + i)]))
        left[i, :] = a

    right = np.zeros((p_b, p_b))
    for i in range(p_b):
        a = np.zeros(p_b)
        for i1 in range(p_b):
            a[i1] = sum_xb_2[i1] - np.sum(div(Zb[:, (i*p_b + i1)]**2, Zb[:, (i*p_b + i)]))
        right[i, :] = a

    mat = np.vstack((mat, np.hstack((left, right, (n - C) * (np.ones((p_b, 1)))))))

    last = np.zeros(p_a + p_b + 1)
    for i1 in range(p_a):
        last[i1] = sum_xa_2[i1] - sum(Za[:, i1]**2) / n
    for i1 in range(p_b):
        last[p_a + i1] = sum_xb_2[i1] - sum(Zb[:, i1]**2) / n
    last[-1] = n - 1
    mat = np.vstack((mat, last.reshape(1, -1)))

    beta_ols = np.linalg.lstsq(X, y, rcond=None)[0]
    r_ols = y - X @ beta_ols

    Za_y = tmat(X_a * np.tile(r_ols, (p_a, 1)).T, fa)
    Zb_y = tmat(X_b * np.tile(r_ols, (p_b, 1)).T, fb)

    right_side = np.array(list(np.sum(div(Za_y**2, Za[:, t_a]), axis=0)) + list(np.sum(div(Zb_y**2, Zb[:, t_b]), axis=0)) + [(np.sum(r_ols)**2) / n])

    right_side = sum(r_ols**2) - right_side

    cov_par = np.linalg.solve(mat, right_side)

    Sigma_a = np.diag(cov_par[:p_a]) if p_a > 1 else cov_par[0:p_a]
    Sigma_b = np.diag(cov_par[p_a:p_a+p_b]) if p_b > 1 else cov_par[p_a:p_a+p_b]
    sigma2e = cov_par[p_a + p_b]

    return {'Sigma_a': Sigma_a, 'Sigma_b' : Sigma_b, 'sigma2e': sigma2e, 'Za': Za, 'Zb': Zb}


def method_of_moments_non_diagonal(y, X, X_a, X_b, fa, fb, diagonal_Sigma_a=False, diagonal_Sigma_b=False):
    n = len(y)
    p = X.shape[1] - 1
    p_a = X_a.shape[1]
    p_b = X_b.shape[1]
    C = len(np.unique(fb))
    R = len(np.unique(fa))

    W_a = np.column_stack([X_a[:, i] * X_a[:, j] for i in range(p_a) for j in range(p_a)])
    Za = tmat(W_a, fa)

    W_b = np.column_stack([X_b[:, i] * X_b[:, j] for i in range(p_b) for j in range(p_b)])
    Zb = tmat(W_b, fb)

    t_a = np.arange(0, p_a**2, p_a + 1)
    t_b = np.arange(0, p_b**2, p_b + 1)

    sum_xa_2 = Za[:, t_a].sum(axis=0)
    sum_xb_2 = Zb[:, t_b].sum(axis=0)

    left = np.zeros((p_a, int(p_a*(p_a+1)/2)))
    for i in range(p_a):
        a = []
        for i1 in range(p_a):
            for j1 in range(i1, p_a):
                if i1 == j1:
                    a += [sum_xa_2[i1] - sum(div(Za[:, (i*p_a + i1)]**2, Za[:, (i*p_a + i)]))]
                else:
                    a += [2 * (sum(Za[:, (i1*p_a + j1)]) - sum(div(Za[:, (i*p_a + j1)]*Za[:, (i*p_a + i1)], Za[:, (i*p_a + i)])))]
        left[i, :] = a

    right = np.zeros((p_a, int(p_b*(p_b+1)/2)))
    for i in range(p_a):
        a = []
        for i1 in range(p_b):
            for j1 in range(i1, p_b):
                if i1 == j1:
                    a += [sum_xb_2[i1] - sum(div(tmat((X_a[:, i] * X_b[:, i1])**2, fa).squeeze(), Za[:, (i*p_a + i)]))]
                else:
                    a += [2 * (sum(Zb[:, (i1*p_b + j1)]) - sum(div(tmat(((X_a[:, i]**2) * X_b[:, i1] * X_b[:, j1]), fa).squeeze(), Za[:, (i*p_a + i)])))]
        right[i, :] = a
    mat = np.hstack((left, right, (n - R)*np.ones((p_a, 1))))

    left = np.zeros((p_b, int(p_a*(p_a+1)/2)))
    for i in range(p_b):
        a = []
        for i1 in range(p_a):
            for j1 in range(i1, p_a):
                if i1 == j1:
                    a += [sum_xa_2[i1] - sum(div(tmat((X_b[:, i] * X_a[:, i1])**2, fb).squeeze(), Zb[:, (i*p_b + i)]))]
                else:
                    a += [2 * (sum(Za[:, (i1*p_a + j1)]) - sum(div(tmat(((X_b[:, i]**2) * X_a[:, i1] * X_a[:, j1]), fb).squeeze(), Zb[:, (i*p_b + i)])))]
        left[i, :] = a

    right = np.zeros((p_b, int(p_b*(p_b+1)/2)))
    for i in range(p_b):
        a = []
        for i1 in range(p_b):
            for j1 in range(i1, p_b):
                if i1 == j1:
                    a += [sum_xb_2[i1] - sum(div(Zb[:, (i*p_b + i1)]**2, Zb[:, (i*p_b + i)]))]
                else:
                    a += [2 * (sum(Zb[:, (i1*p_b + j1)]) - sum(div(Zb[:, (i*p_b + j1)]*Zb[:, (i*p_b + i1)], Zb[:, (i*p_b + i)])))]
        right[i, :] = a

    mat = np.vstack((mat, np.hstack((left, right, (n - C)*np.ones((p_b, 1))))))

    last = []
    for i1 in range(p_a):
        for j1 in range(i1, p_a):
            if i1 == j1:
                last += [sum_xa_2[i1] - sum(Za[:, i1]**2) / n]
            else:
                last += [2 * sum(Za[:, (i1*p_a + j1)]) - 2 * sum((Za[:, i1]*Za[:, j1])) / n]
    for i1 in range(p_b):
        for j1 in range(i1, p_b):
            if i1 == j1:
                last += [sum_xb_2[i1] - sum(Zb[:, i1]**2) / n]
            else:
                last += [2 * sum(Zb[:, (i1*p_b + j1)]) - 2 * sum((Zb[:, i1]*Zb[:, j1])) / n]
    last += [n - 1]
    mat = np.vstack((mat, last))

    beta_ols = np.linalg.lstsq(X, y, rcond=None)[0]
    r_ols = y - X @ beta_ols

    Za_y = tmat(X_a * np.tile(r_ols, (p_a, 1)).T, fa)
    Zb_y = tmat(X_b * np.tile(r_ols, (p_b, 1)).T, fb)

    right_side = np.sum(r_ols**2) - np.array(list(np.sum(div(Za_y**2, Za[:, t_a]), axis=0)) +
                                                    list(np.sum(div(Zb_y**2, Zb[:, t_b]), axis=0)) +
                                                    [(np.sum(r_ols)**2) / n])
    m = np.mean(mat)
    mat = mat / m
    right_side = right_side / m
    sequence_a = np.concatenate([((i-1)*p_a) + np.arange(i-1, p_a) for i in range(1, p_a+1)])
    sequence_b = np.concatenate([((i-1)*p_b) + np.arange(i-1, p_b) for i in range(1, p_b+1)])

    sequence_a_diag = np.concatenate([((i-1)*p_a) + np.arange(i, p_a) for i in range(1, p_a+1)])
    sequence_b_diag = np.concatenate([((i-1)*p_b) + np.arange(i, p_b) for i in range(1, p_b+1)])

    Sigma_a = cp.Variable((p_a, p_a), PSD=True)
    Sigma_b = cp.Variable((p_b, p_b), PSD=True)
    sigma2e = cp.Variable()

    constraints = [Sigma_a >> 0, Sigma_a == Sigma_a.T, Sigma_b >> 0, sigma2e >= 0, Sigma_b == Sigma_b.T,
                   mat[:, :int(p_a*(p_a + 1)/2)] @ cp.vec(Sigma_a)[sequence_a] +
                   mat[:, int(p_a*(p_a + 1)/2):int(((p_a)*(p_a + 1))/2 + ((p_b)*(p_b + 1))/2)] @ cp.vec(Sigma_b)[sequence_b] +
                   sigma2e*mat[:, int(((p_a)*(p_a + 1))/2 + ((p_b)*(p_b + 1))/2)] == right_side]

    if diagonal_Sigma_a:
        constraints.append(cp.vec(Sigma_a)[sequence_a_diag] == 0)

    if diagonal_Sigma_b:
        constraints.append(cp.vec(Sigma_b)[sequence_b_diag] == 0)

    objective = 0
    prob = cp.Problem(cp.Minimize(objective), constraints)
    result = prob.solve(solver='MOSEK')

    if prob.status == 'optimal':
        return {'result_status': prob.status,
                'Sigma_a': Sigma_a.value,
                'Sigma_b': Sigma_b.value,
                'sigma2e': sigma2e.value,
                'Za': Za,
                'Zb': Zb}
    else:
        print("Method of moments approach failed")
        return {'result_status': 'Failed',
                'Za': Za,
                'Zb': Zb}



def vanilla_backfitting(y, X, X_a, X_b, fa, fb, Sigma_a, Sigma_b, sigma2e, Za, Zb, max_iter=600, conv_thres=1e-6, device = 'cpu'):
    p_a = X_a.shape[1]
    p_b = X_b.shape[1]
    C = len(np.unique(fb))
    R = len(np.unique(fa))
    lam = sigma2e * np.linalg.solve(Sigma_a, np.eye(p_a))
    alpha = sigma2e * np.linalg.solve(Sigma_b, np.eye(p_b))

    X_a = torch.tensor(X_a, device=device)
    X_b = torch.tensor(X_b, device=device)
    y = torch.tensor(y, device = device)


    def xtx_lam_inverse(i):
        # Reshape Za[i, :] to be a p_a x p_a matrix
        matrix_za_i = np.reshape(Za[i, :], (p_a, p_a))
        # Compute the inverse of the matrix sum
        result = np.linalg.inv(matrix_za_i + lam)
        return result

    def xtx_alpha_inverse(i):
        # Reshape Za[i, :] to be a p_a x p_a matrix
        matrix_zb_i = np.reshape(Zb[i, :], (p_b, p_b))
        # Compute the inverse of the matrix sum
        result = np.linalg.inv(matrix_zb_i + alpha)
        return result
    projection_a = torch.tensor(np.array([xtx_lam_inverse(i) for i in range(R)]), device = device)
    projection_b = torch.tensor(np.array([xtx_alpha_inverse(i) for i in range(C)]), device = device)


    def rslope_a(r):

        # Create the Rvec matrix
        Rvec = X_a * r.unsqueeze(1).repeat(1, p_a)

        # Compute X_R using the tmat function
        X_R = tmat(Rvec, fa)

        # Define the pr function
        def pr(i):
            return (projection_a[i] @ X_R[i, :])


        beta = torch.tensor(np.array(list(map(pr, range(R)))), device = device)

        # Convert beta back to torch tensor and move to GPU

        # Compute the fit values
        fit = torch.sum(X_a * beta[ fa, :], axis=1)

        return beta, fit

    def rslope_b(r):

        # Create the Rvec matrix
        Rvec = X_b * r.unsqueeze(1).repeat(1, p_b)

        # Compute X_R using the tmat function
        X_R = tmat(Rvec, fb)

        # Define the pr function
        def pr(i):
            return (projection_b[i] @ X_R[i, :])


        beta = torch.tensor(np.array(list(map(pr, range(C)))), device = device)

        # Convert beta back to torch tensor and move to GPU

        # Compute the fit values
        fit = torch.sum(X_b * beta[ fb, :], axis=1)

        # Move beta back to CPU for the return value
        return beta, fit

    linear_model.fit(X, y)
    fvo = linear_model.predict(X)
    faxo = fbxo = fvo * 0
    diff = 1
    iter = 1

    while iter <= max_iter and diff >= conv_thres:
        r = y - fvo - fbxo
        beta_a, fax = rslope_a(r)
        r = y - fvo - fax
        beta_b, fbx = rslope_b(r)
        r = y - fax - fbx
        linear_model.fit(X, r)
        fv = linear_model.predict(X)
        n0 = norm(fv - fvo) / norm(fv)
        na = norm(fax - faxo) / norm(fax)
        nb = norm(fbx - fbxo) / norm(fbx)
        diff = max(n0, na, nb)
        print(iter, "max change:", diff)
        fvo = fv
        faxo = fax
        fbxo = fbx
        iter += 1

    return {'beta': linear_model.coef_, 'niter': iter, 'A': beta_a, 'B': beta_b}


def variational_backfitting(y, X, X_a, X_b, fa, fb, Sigma_a, Sigma_b, sigma2e, Za, Zb, max_iter=600, conv_thres=1e-6, covariances_diag_Sigma_a=False, covariances_diag_Sigma_b=False, device = 'cpu'):
    p_a = X_a.shape[1]
    p_b = X_b.shape[1]
    C = len(np.unique(fb))
    R = len(np.unique(fa))
    n = len(y)
    lam = sigma2e * np.linalg.solve(Sigma_a, np.eye(p_a))
    alpha = sigma2e * np.linalg.solve(Sigma_b, np.eye(p_b))

    X_a = torch.tensor(X_a, device=device)
    X_b = torch.tensor(X_b, device=device)
    y = torch.tensor(y, device = device)

    def xtx_lam_inverse(i, lam):
        matrix_za_i = np.reshape(Za[i, :], (p_a, p_a))
        # Compute the inverse of the matrix sum
        result = np.linalg.inv(matrix_za_i + lam)
        return result

    def xtx_alpha_inverse(i, alpha):
        matrix_zb_i = np.reshape(Zb[i, :], (p_b, p_b))
        # Compute the inverse of the matrix sum
        result = np.linalg.inv(matrix_zb_i + alpha)
        return result

    def rslope_a(r, xtx_lam_inv):
        Rvec = X_a * r.unsqueeze(1).repeat(1, p_a)
        X_R = tmat(Rvec, fa)
        def pr(i):
            return (xtx_lam_inv[i] @ X_R[i, :])
        beta = torch.tensor(np.array(list(map(pr, range(R)))), device = device)
        fit = torch.sum(X_a * beta[ fa, :], axis=1)

        # Move beta back to CPU for the return value
        return beta, fit

    def rslope_b(r, xtx_alpha_inv):
        Rvec = X_b * r.unsqueeze(1).repeat(1, p_b)
        X_R = tmat(Rvec, fb)
        def pr(i):
            return (xtx_alpha_inv[i] @ X_R[i, :])
        beta = torch.tensor(np.array(list(map(pr, range(C)))), device = device)
        fit = torch.sum(X_b * beta[ fb, :], axis=1)

        # Move beta back to CPU for the return value
        return beta, fit

    linear_model.fit(X, y)
    fvo = linear_model.predict(X)
    faxo = fbxo = fvo*0
    diff = 1
    iter = 1

    while iter <= max_iter and diff >= conv_thres:
        r = y - fvo - fbxo
        xtx_lam_inv = [xtx_lam_inverse(i, lam) for i in range(R)]
        xtx_alpha_inv = [xtx_alpha_inverse(i, alpha) for i in range(C)]
        a, fax = rslope_a(r, xtx_lam_inv)
        Sigma_a = (sum([np.outer(a[i, :], a[i, :]) for i in range(R)]) + sigma2e * sum(xtx_lam_inv)) / R
        if p_a > 1 and covariances_diag_Sigma_a:
            Sigma_a = np.diag(np.diag(Sigma_a))
        r = y - fvo - fax
        b, fbx = rslope_b(r, xtx_alpha_inv)
        Sigma_b = (sum([np.outer(b[i, :], b[i, :]) for i in range(C)]) + sigma2e * sum(xtx_alpha_inv)) / C
        if p_b > 1 and covariances_diag_Sigma_b:
            Sigma_b = np.diag(np.diag(Sigma_b))
        r = y - fax - fbx
        linear_model.fit(X, r)
        fv = linear_model.predict(X)
        r = y - fv - fax - fbx
        a_values = torch.tensor([np.trace(Za[i, :].reshape(p_a, p_a) @ xtx_lam_inv[i]) for i in range(R)])
        b_values = torch.tensor([np.trace(Zb[i, :].reshape(p_b, p_b) @ xtx_alpha_inv[i]) for i in range(C)])
        sigma2e = (sigma2e * np.array(torch.sum(a_values) + torch.sum(b_values)) + np.array(torch.sum(r**2))) / n

        n0 = norm(fv - fvo) / norm(fv)
        na = norm(fax - faxo) / norm(fax)
        nb = norm(fbx - fbxo) / norm(fbx)
        diff = max(n0, na, nb)
        print(iter, "max change:", diff)
        fvo = fv
        faxo = fax
        fbxo = fbx
        iter += 1
        lam = sigma2e * np.linalg.solve(Sigma_a, np.eye(p_a))
        alpha = sigma2e * np.linalg.solve(Sigma_b, np.eye(p_b))

    return {'beta': beta, 'niter': iter, 'A': a, 'B': b, 'Sigma_a': Sigma_a, 'Sigma_b': Sigma_b, 'sigma2e': sigma2e}


def clubbed_backfitting(y, X, X_a, X_b, fa, fb, Sigma_a, Sigma_b, sigma2e, Za, Zb, max_iter=600, conv_thres=1e-6):
    p_a = X_a.shape[1]
    p_b = X_b.shape[1]
    n = len(y)
    p = X.shape[1] - 1
    C = len(np.unique(fb))
    R = len(np.unique(fa))
    lam = sigma2e * np.linalg.solve(Sigma_a, np.eye(p_a))
    alpha = sigma2e * np.linalg.solve(Sigma_b, np.eye(p_b))

    X = torch.tensor(X, device=device)
    X_a = torch.tensor(X_a, device=device)
    X_b = torch.tensor(X_b, device=device)
    y = torch.tensor(y, device = device)


    def xtx_lam_inverse(i):
        return np.linalg.solve(Za[i, :].reshape(p_a, p_a) + lam, np.eye(p_a))

    def xtx_alpha_inverse(i):
        return np.linalg.solve(Zb[i, :].reshape(p_b, p_b) + alpha, np.eye(p_b))

    projection_a = torch.tensor(np.array([xtx_lam_inverse(i) for i in range(R)]), device = device)
    projection_b = torch.tensor(np.array([xtx_alpha_inverse(i) for i in range(C)]), device = device)
    def rslope_a(r):

        # Create the Rvec matrix
        Rvec = X_a * r.unsqueeze(1).repeat(1, p_a)

        # Compute X_R using the tmat function
        X_R = tmat(Rvec, fa)

        # Define the pr function
        def pr(i):
            return (projection_a[i] @ X_R[i, :])

        beta = torch.tensor(np.array(list(map(pr, range(R)))), device = device)

        # Convert beta back to torch tensor and move to GPU

        # Compute the fit values
        fit = torch.sum(X_a * beta[ fa, :], axis=1)

        return beta, fit

    def rslope_b(r):

        # Create the Rvec matrix
        Rvec = X_b * r.unsqueeze(1).repeat(1, p_b)

        # Compute X_R using the tmat function
        X_R = tmat(Rvec, fb)

        # Define the pr function
        def pr(i):
            return (projection_b[i] @ X_R[i, :])


        beta = torch.tensor(np.array(list(map(pr, range(C)))), device = device)

        # Convert beta back to torch tensor and move to GPU

        # Compute the fit values
        fit = torch.sum(X_b * beta[ fb, :], axis=1)

        # Move beta back to CPU for the return value
        return beta, fit

    def club_a(r):
        Xresid = X.detach().clone()
        for i in range(X.shape[1]):
            Xresid[:, i] = X[:, i] - rslope_a(X[:, i])[1]
        beta = np.linalg.solve(X.T @ Xresid, Xresid.T @ r)
        fv = X @ beta
        betaa, fax = rslope_a(r - fv)
        return betaa, fax

    def club_b(r):
        Xresid = X.detach().clone()
        for i in range(X.shape[1]):
            Xresid[:, i] = X[:, i] - rslope_b(X[:, i])[1]
        beta = np.linalg.solve(X.T @ Xresid, Xresid.T @ r)
        fv = X @ beta
        betab, fbx = rslope_b(r - fv)
        return beta, fv, betab, fbx

    fvo = faxo = fbxo = np.zeros_like(y)
    diff = 1
    iter = 1
    while iter <= max_iter and diff >= conv_thres:
        r = y - fbxo
        betaa, fax  = club_a(r)
        r = y - fax
        beta, fv, betab, fbx = club_b(r)
        n0 = norm(fv - fvo) / norm(fv)
        na = norm(fax - faxo) / norm(fax)
        nb = norm(fbx - fbxo) / norm(fbx)
        diff = max(n0, na, nb)
        print(iter, "max change:", diff)
        fvo = fv
        faxo = fax
        fbxo = fbx
        iter += 1

    return {'beta': beta, 'niter': iter, 'A': betaa, 'B': betab}


## device = 'cuda' will work here too
def clubbed_variational_backfitting(y, X, X_a, X_b, fa, fb, Sigma_a, Sigma_b, sigma2e, Za, Zb, max_iter=600, conv_thres=1e-6, covariances_diag_Sigma_a=False, covariances_diag_Sigma_b=False, device = 'cpu'):


	p_a = X_a.shape[1]
	p_b = X_b.shape[1]
	p = X.shape[1] - 1
	n = len(y)
	C = len(np.unique(fb))
	R = len(np.unique(fa))
	Sigma_a = torch.tensor(Sigma_a, dtype=torch.float64, device=device)
	Sigma_b = torch.tensor(Sigma_b, dtype=torch.float64, device=device)
	sigma2e = torch.tensor(sigma2e, dtype=torch.float64, device=device)



	lam = sigma2e * torch.linalg.inv(Sigma_a)
	alpha = sigma2e * torch.linalg.inv(Sigma_b)

	X = torch.tensor(X, device=device)
	X_a = torch.tensor(X_a, device=device, dtype = torch.float64)
	X_b = torch.tensor(X_b, device=device, dtype = torch.float64)
	y = torch.tensor(y, device = device, dtype = torch.float64)
	Za = torch.tensor(Za, dtype = torch.float64, device=device)
	Zb = torch.tensor(Zb, dtype = torch.float64, device=device)


	def xtx_lam_inverse(i, lam):
		matrix_za_i = torch.reshape(Za[i, :], (p_a, p_a))
		# Compute the inverse of the matrix sum
		result = torch.linalg.inv(matrix_za_i + lam )
		return result

	def xtx_alpha_inverse(i, alpha):
		matrix_zb_i = torch.reshape(Zb[i, :], (p_b, p_b))
		# Compute the inverse of the matrix sum
		result = torch.linalg.inv(matrix_zb_i + alpha)
		return result

	def rslope_a(r, xtx_lam_inv):
		Rvec = X_a * r.unsqueeze(1).repeat(1, p_a)
		X_R = torch.tensor(tmat(Rvec.cpu(), fa), device = device, dtype = torch.float64)
		def pr(i):
			return (xtx_lam_inv[i] @ X_R[i, :])
		beta = torch.stack(list(map(pr, range(R)))).cuda()
		fit = torch.sum(X_a * beta[ fa, :], axis=1)

		# Move beta back to CPU for the return value
		return beta, fit

	def rslope_b(r, xtx_alpha_inv):
		Rvec = X_b * r.unsqueeze(1).repeat(1, p_b)
		X_R = torch.tensor(tmat(Rvec.cpu(), fb), device = device, dtype = torch.float64)
		def pr(i):
			return (xtx_alpha_inv[i] @ X_R[i, :])
		beta = torch.stack(list(map(pr, range(C)))).cuda()
		fit = torch.sum(X_b * beta[ fb, :], axis=1)
		return beta, fit

	def club_a(r, xtx_lam_inv):
		Xresid = X.detach().clone()
		for i in range(X.shape[1]):
			Xresid[:, i] = X[:, i] - rslope_a(X[:, i], xtx_lam_inv)[1]
		beta = torch.linalg.solve(X.T @ Xresid, Xresid.T @ r)
		fv = X @ beta
		betaa, fax = rslope_a(r - fv, xtx_lam_inv)
		return betaa, fax

	def club_b(r, xtx_alpha_inv):
		Xresid = X.detach().clone()
		for i in range(X.shape[1]):
			Xresid[:, i] = X[:, i] - rslope_b(X[:, i], xtx_alpha_inv)[1]
		beta = torch.linalg.solve(X.T @ Xresid, Xresid.T @ r)
		fv = X @ beta
		betab, fbx = rslope_b(r - fv, xtx_alpha_inv)
		return beta, fv, betab, fbx

	fvo = faxo = fbxo = y*0
	iter = 1
	diff = 1
	Changes_sigma2e = []
	Changes_Sigma_a = []
	Changes_Sigma_b = []

	if sigma2e is None:
		sigma2e = 1
	if Sigma_a is None:
		Sigma_a = torch.eye(p_a, dtype = torch.float64, device = device)
	if Sigma_b is None:
		Sigma_b = torch.eye(p_b, dtype = torch.float64, device = device)
	old_sigma2e = sigma2e
	old_Sigma_a = Sigma_a
	old_Sigma_b = Sigma_b

	lam = sigma2e * torch.linalg.inv(Sigma_a)
	alpha = sigma2e * torch.linalg.inv(Sigma_b)

	training_error = []

	while iter <= max_iter and diff >= conv_thres:
		xtx_lam_inv = [xtx_lam_inverse(i, lam) for i in range(R)]
		xtx_alpha_inv = [xtx_alpha_inverse(i, alpha) for i in range(C)]
		r = y - fbxo
		a, fax = club_a(r, xtx_lam_inv)
		new_Sigma_a = (torch.tensor(sum([np.outer(a[i, :].cpu(), a[i, :].cpu()) for i in range(R)]), device = device) + sigma2e * sum(xtx_lam_inv)) / R
		if p_a > 1 and covariances_diag_Sigma_a:
			new_Sigma_a = torch.diag(torch.diag(new_Sigma_a))
		r = y - fax
		beta, fv, b, fbx = club_b(r, xtx_alpha_inv)
		new_Sigma_b = (torch.tensor(sum([np.outer(b[i, :].cpu(), b[i, :].cpu()) for i in range(C)]), device = device) + sigma2e * sum(xtx_alpha_inv)) / C
		if p_b > 1 and covariances_diag_Sigma_b:
			new_Sigma_b = torch.diag(torch.diag(new_Sigma_b))

		n0 = torch.norm(fv - fvo) / torch.norm(fv)
		na = torch.norm(fax - faxo) / torch.norm(fax)
		nb = torch.norm(fbx - fbxo) / torch.norm(fbx)
		diff = max(n0, na, nb)
		fvo = fv
		faxo = fax
		fbxo = fbx
		r = y - fv - fax - fbx
		a_values = torch.tensor([torch.trace(Za[i, :].reshape(p_a, p_a) @ xtx_lam_inv[i]) for i in range(R)])
		b_values = torch.tensor([torch.trace(Zb[i, :].reshape(p_b, p_b) @ xtx_alpha_inv[i]) for i in range(C)])
		new_sigma2e = (old_sigma2e * (torch.sum(a_values) + torch.sum(b_values)) + torch.sum(r**2)) / n

		lam = new_sigma2e * torch.linalg.inv(new_Sigma_a)
		alpha = new_sigma2e * torch.linalg.inv(new_Sigma_b)

		change_sigma2e = abs(new_sigma2e / old_sigma2e - 1)
		change_Sigma_a = torch.norm(new_Sigma_a - old_Sigma_a)
		change_Sigma_b = torch.norm(new_Sigma_b - old_Sigma_b)
		Changes_sigma2e.append(change_sigma2e)
		Changes_Sigma_a.append(change_Sigma_a)
		Changes_Sigma_b.append(change_Sigma_b)
		old_sigma2e = new_sigma2e
		old_Sigma_a = new_Sigma_a
		old_Sigma_b = new_Sigma_b
		training_error.append(torch.mean(r ** 2))
		print(iter, "max change:", diff)
		iter += 1
		results = {'beta': beta, 'Sigma_a': new_Sigma_a, 'Sigma_b': new_Sigma_b, 'sigma2e': new_sigma2e, 'niter': iter - 1, 'A': a, 'B': b, 'training_error': training_error, 'Changes_sigma2e': Changes_sigma2e, 'Changes_Sigma_a': Changes_Sigma_a, 'Changes_Sigma_b': Changes_Sigma_b}

	return results



# Function to fit on random effects
def fit_on_random_effects(y, X_a, X_b, fa, fb, Sigma_a, Sigma_b, sigma2e, max_iter=600, conv_thres=1e-6, device = 'cpu'):
    p_a = X_a.shape[1]
    p_b = X_b.shape[1]
    C = len(np.unique(fb))
    R = len(np.unique(fa))

    lam = sigma2e * np.linalg.inv(Sigma_a)
    alpha = sigma2e * np.linalg.inv(Sigma_b)

    X_a = torch.tensor(X_a, device=device)
    X_b = torch.tensor(X_b, device=device)
    y = torch.tensor(y, device = device)

    # Construct W_a and W_b
    W_a = np.column_stack([X_a[:, i] * X_a[:, j] for i in range(p_a) for j in range(p_a)])
    W_b = np.column_stack([X_b[:, i] * X_b[:, j] for i in range(p_b) for j in range(p_b)])

    # Create Za and Zb
    Za = tmat(W_a, fa)
    Zb = tmat(W_b, fb)

    # Define xtx_lam_inverse and xtx_alpha_inverse functions
    def xtx_lam_inverse(i):
        return np.linalg.inv(Za[i].reshape(p_a, p_a) + lam)

    def xtx_alpha_inverse(i):
        return np.linalg.inv(Zb[i].reshape(p_b, p_b) + alpha)

    projection_a = torch.tensor(np.array([xtx_lam_inverse(i) for i in range(R)]), device = device)
    projection_b = torch.tensor(np.array([xtx_alpha_inverse(i) for i in range(C)]), device = device)


    # Define rslope_a and rslope_b functions
    def rslope_a(r):
        Rvec = X_a * r.unsqueeze(1).repeat(1, p_a)
        X_R = tmat(Rvec, fa)

        # Define the pr function
        def pr(i):
            return (projection_a[i] @ X_R[i, :])
        beta = torch.tensor(np.array(list(map(pr, range(R)))), device = device)
        fit = torch.sum(X_a * beta[ fa, :], axis=1)
        return fit

    def rslope_b(r):
        Rvec = X_a * r.unsqueeze(1).repeat(1, p_b)
        X_R = tmat(Rvec, fb)

        # Define the pr function
        def pr(i):
            return (projection_b[i] @ X_R[i, :])
        beta = torch.tensor(np.array(list(map(pr, range(C)))), device = device)
        fit = torch.sum(X_b * beta[ fb, :], axis=1)
        return fit

    faxo = fbxo = np.zeros_like(y)
    diff = 1
    iter = 1

    while iter <= max_iter and diff >= conv_thres:
        r = y - fbxo
        fax = rslope_a(r)
        r = y - fax
        fbx = rslope_b(r)
        na = np.linalg.norm(fax - faxo, 2) / np.linalg.norm(fax, 2)
        nb = np.linalg.norm(fbx - fbxo, 2) / np.linalg.norm(fbx, 2)
        diff = max(na, nb)
        print(iter, "max change:", diff)
        faxo = fax
        fbxo = fbx
        iter += 1

    return {'fax': fax, 'fbx': fbx}

# Function to calculate covariances of beta hat
def covariances_beta_hat(y, X, X_a, X_b, fa, fb, Sigma_a, Sigma_b, sigma2e, X_residual=None):
    p_a = X_a.shape[1]
    p_b = X_b.shape[1]
    q = X.shape[1]
    X = torch.tensor(X)
    if X_residual is None:
        X_residual = X.detach().clone()
        for k in range(q):
            fit_random_effects = fit_on_random_effects(X[:, k], X_a, X_b, fa, fb, Sigma_a, Sigma_b, sigma2e, max_iter=100, conv_thres=1e-6)
            X_residual[:, k] = X_residual[:, k] - fit_random_effects['fax'] - fit_random_effects['fbx']

    X_a_Sigma_a_half = X_a @ sqrtm(Sigma_a)
    X_b_Sigma_b_half = X_b @ sqrtm(Sigma_b)
    X_t_X_res_inverse = np.linalg.inv(X.T @ X_residual)

    X_res_X_a_Sigma_a_half = np.column_stack([X_residual[:, i] * X_a_Sigma_a_half[:, j] for i in range(q) for j in range(p_a)])
    X_res_X_a_Sigma_a_half_sum = tmat(X_res_X_a_Sigma_a_half, fa)

    final_a = np.zeros((q, q))
    for i in range(q):
        for j in range(i, q):
            final_a[i, j] = np.sum(X_res_X_a_Sigma_a_half_sum[:, (i*p_a):((i+1)*p_a)] * X_res_X_a_Sigma_a_half_sum[:, (j*p_a):((j+1)*p_a)])
            final_a[j, i] = final_a[i, j]

    X_res_X_b_Sigma_b_half = np.column_stack([X_residual[:, i] * X_b_Sigma_b_half[:, j] for i in range(q) for j in range(p_b)])
    X_res_X_b_Sigma_b_half_sum = tmat(X_res_X_b_Sigma_b_half, fb)

    final_b = np.zeros((q, q))
    for i in range(q):
        for j in range(i, q):
            final_b[i, j] = np.sum(X_res_X_b_Sigma_b_half_sum[:, (i*p_b):((i+1)*p_b)] * X_res_X_b_Sigma_b_half_sum[:, (j*p_b):((j+1)*p_b)])
            final_b[j, i] = final_b[i, j]

    covariance_gls_beta_hat_gls = X_t_X_res_inverse @ (final_a + final_b + sigma2e * np.array(X_residual.T @ X_residual)) @ X_t_X_res_inverse
    return {'covariance_gls_beta_hat_gls': covariance_gls_beta_hat_gls}


def scalable_crossed_random(y, X, X_a, X_b, fa, fb, Sigma_a_diagonal = False, Sigma_b_diagonal = False,
                            variational = True, clubbing = True, method_of_mom = False, beta_covariance = False,
                            max_iter=600, conv_thres=1e-6, device = 'cuda'):
    n = len(y)

    range_f1_train = sorted(np.unique(fa))
    range_f2_train = sorted(np.unique(fb))

    R = len(range_f1_train)
    C = len(range_f2_train)

    bij_f1 = {value: idx for idx, value in enumerate(range_f1_train, start=0)}
    bij_f2 = {value: idx for idx, value in enumerate(range_f2_train, start=0)}

    fa = np.array([bij_f1[val] for val in fa])
    fb = np.array([bij_f2[val] for val in fb])

    p = X.shape[1] - 1
    p_a = X_a.shape[1]
    p_b = X_b.shape[1]

    if (Sigma_a_diagonal and Sigma_b_diagonal) or (p_a == 1 and p_b == 1):
      if method_of_mom:
          mom = method_of_moments_diagonal(y, X, X_a, X_b, fa, fb)
          if np.all(np.diag(mom['Sigma_a']) > 0) and np.all(np.diag(mom['Sigma_b']) > 0) and mom['sigma2e'] > 0 and method_of_mom:
              print("Method of moments approach worked")
              Za, Zb = mom['Za'], mom['Zb']
              mm_Sigma_a, mm_Sigma_b, mm_sigma2e = mom['Sigma_a'], mom['Sigma_b'], mom['sigma2e']
              mom_status = 1
          else:
              print("Method of moments approach failed")
              mm_Sigma_a, mm_Sigma_b = np.eye(p_a), np.eye(p_b)
              mm_sigma2e = 1
              Za, Zb = mom['Za'], mom['Zb']
              mom_status = 0
      else:
            # Construct W_a and W_b
            W_a = np.column_stack([X_a[:, i] * X_a[:, j] for i in range(p_a) for j in range(p_a)])
            W_b = np.column_stack([X_b[:, i] * X_b[:, j] for i in range(p_b) for j in range(p_b)])

            # Create Za and Zb
            Za = tmat(W_a, fa)
            Zb = tmat(W_b, fb)
            mm_Sigma_a = np.eye(p_a)
            mm_Sigma_b = np.eye(p_b)
            mm_sigma2e = 1
    else:
      if method_of_mom:
        mom = method_of_moments_non_diagonal(y, X, X_a, X_b, fa, fb, diagonal_Sigma_a=Sigma_a_diagonal, diagonal_Sigma_b=Sigma_b_diagonal)
        if mom['result_status'] == 'optimal' and method_of_mom:
            print("Method of moments approach worked")
            mom_status = 1
            mm_Sigma_a, mm_Sigma_b, mm_sigma2e = mom['Sigma_a'], mom['Sigma_b'], mom['sigma2e']
            Za, Zb = mom['Za'], mom['Zb']
        else:
            print("Method of moments approach failed")
            mm_Sigma_a, mm_Sigma_b = np.eye(p_a), np.eye(p_b)
            mm_sigma2e = 1
            Za, Zb = mom['Za'], mom['Zb']
            mom_status = 0
      else:
        # Construct W_a and W_b
        W_a = np.column_stack([X_a[:, i] * X_a[:, j] for i in range(p_a) for j in range(p_a)])
        W_b = np.column_stack([X_b[:, i] * X_b[:, j] for i in range(p_b) for j in range(p_b)])

        # Create Za and Zb
        Za = tmat(W_a, fa)
        Zb = tmat(W_b, fb)

        mm_Sigma_a = np.eye(p_a)
        mm_Sigma_b = np.eye(p_b)
        mm_sigma2e = 1


    if variational or mom_status == 0:
        if clubbing:
            fit = clubbed_variational_backfitting(y, X, X_a, X_b, fa, fb, mm_Sigma_a, mm_Sigma_b, mm_sigma2e, Za, Zb,
                                                  covariances_diag_Sigma_a=Sigma_a_diagonal, covariances_diag_Sigma_b=Sigma_b_diagonal,
                                                  conv_thres=conv_thres, max_iter=max_iter, device = device)
        else:
            fit = variational_backfitting(y, X, X_a, X_b, fa, fb, mm_Sigma_a, mm_Sigma_b, mm_sigma2e, Za, Zb,
                                          covariances_diag_Sigma_a=Sigma_a_diagonal, covariances_diag_Sigma_b=Sigma_b_diagonal,
                                          conv_thres=conv_thres, max_iter=max_iter, device = device)

        results = {
            'fixed_effects': fit['beta'],
            'cov_fa': fit['Sigma_a'],
            'cov_fb': fit['Sigma_b'],
            'sigma': np.sqrt(fit['sigma2e'].cpu()),
            'random_effects_fa': fit['A'],
            'random_effects_fb': fit['B']
        }
    else:
        if clubbing:
            fit = clubbed_backfitting(y, X, X_a, X_b, fa, fb, mm_Sigma_a, mm_Sigma_b, mm_sigma2e, Za, Zb,
                                      conv_thres=conv_thres, max_iter=max_iter, device = device)
        else:
            fit = vanilla_backfitting(y, X, X_a, X_b, fa, fb, mm_Sigma_a, mm_Sigma_b, mm_sigma2e, Za, Zb,
                                      conv_thres=conv_thres, max_iter=max_iter, device = device)

        results = {
            'fixed_effects': fit['beta'],
            'cov_fa': mm_Sigma_a,
            'cov_fb': mm_Sigma_b,
            'sigma': np.sqrt(mm_sigma2e),
            'random_effects_fa': fit['A'],
            'random_effects_fb': fit['B']
        }

    if beta_covariance:
        covariance_beta_hat = covariances_beta_hat(y, X, X_a, X_b, fa, fb, results['cov_fa'], results['cov_fb'], results['sigma']**2, X_residual=None)
        results['covariance_beta_hat'] = covariance_beta_hat['covariance_gls_beta_hat_gls']

    return results

fit = scalable_crossed_random(y, X, X, X, f1, f2)

def naivety_inefficiency_wrt_ols(y, X, X_a, X_b, fa, fb, fit, X_residual=None):
    q = X.shape[1]
    p_a = X_a.shape[1]
    p_b = X_b.shape[1]
    fit_ols = sm.OLS(y, X).fit()
    ols_sigma2e = fit_ols.mse_resid
    X = torch.tensor(X)

    X_t_X_inverse = np.linalg.inv(X.T @ X)

    if X_residual is None:
        X_residual = X.detach().clone()

        for k in range(q):
            fit_random_effects = fit_on_random_effects(X[:, k], X_a, X_b, fa, fb, np.array(fit['cov_fa'].cpu()), np.array(fit['cov_fb'].cpu()), np.array(fit['sigma']**2), max_iter=800, conv_thres=1e-6)
            X_residual[:, k] = X[:, k] - fit_random_effects['fax'] - fit_random_effects['fbx']
    if fit.get('covariance_beta_hat') is None:
        cov = covariances_beta_hat(y, X, X_a, X_b, fa, fb, np.array(fit['cov_fa'].cpu()), np.array(fit['cov_fb'].cpu()), np.array(fit['sigma']**2), X_residual=X_residual)
        covariance_gls_beta_hat_gls = cov['covariance_gls_beta_hat_gls']
    else:
        covariance_gls_beta_hat_gls = fit['covariance_beta_hat']

    X_t_X_res_inverse = np.linalg.inv(X.T @ X_residual)
    covariance_ols_beta_hat_gls = (ols_sigma2e * X_t_X_res_inverse) @ np.array(X_residual.T @ X_residual) @ X_t_X_res_inverse

    X_a_Sigma_a_half = X_a @ sqrtm(np.array(fit['cov_fa'].cpu()))
    X_b_Sigma_b_half = X_b @ sqrtm(np.array(fit['cov_fb'].cpu()))
    X_X_a_Sigma_a_half = np.hstack([X[:, i:i+1] * X_a_Sigma_a_half for i in range(q)])
    X_X_a_Sigma_a_half_sum = tmat(X_X_a_Sigma_a_half, fa)

    final_a = np.zeros((q, q))
    for i in range(q):
        for j in range(i, q):
            final_a[i, j] = np.sum(X_X_a_Sigma_a_half_sum[:, (i*p_a):((i+1)*p_a)] * X_X_a_Sigma_a_half_sum[:, (j*p_a):((j+1)*p_a)])
            final_a[j, i] = final_a[i, j]

    X_X_b_Sigma_b_half = np.hstack([X[:, i:i+1] * X_b_Sigma_b_half for i in range(q)])
    X_X_b_Sigma_b_half_sum = tmat(X_X_b_Sigma_b_half, fb)
    final_b = np.zeros((q, q))
    for i in range(q):
        for j in range(i, q):
            final_b[i, j] = np.sum(X_X_b_Sigma_b_half_sum[:, (i*p_b):((i+1)*p_b)] * X_X_b_Sigma_b_half_sum[:, (j*p_b):((j+1)*p_b)])
            final_b[j, i] = final_b[i, j]

    covariance_gls_beta_hat_ols = X_t_X_inverse @ (final_a + final_b + np.array(fit['sigma']**2) * np.array(X.T @ X)) @ X_t_X_inverse
    covariance_ols_beta_hat_ols = ols_sigma2e * X_t_X_inverse
    naivety = np.diag(covariance_gls_beta_hat_ols) / np.diag(covariance_ols_beta_hat_ols)
    inefficiency = np.diag(covariance_gls_beta_hat_ols) / np.diag(covariance_gls_beta_hat_gls)

    return {
        'X_residual': X_residual,
        'covariance_ols_beta_hat_ols': covariance_ols_beta_hat_ols,
        'covariance_gls_beta_hat_ols': covariance_gls_beta_hat_ols,
        'covariance_ols_beta_hat_gls': covariance_ols_beta_hat_gls,
        'covariance_gls_beta_hat_gls': covariance_gls_beta_hat_gls,
        'naivety': naivety,
        'inefficiency': inefficiency
    }

